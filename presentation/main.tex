\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tikz}

\usetheme{Madrid}
\usecolortheme{default}

\title{Neural Networks for Inverse Kinematics}
\subtitle{Demonstrating the Universal Approximation Theorem}
\author{EE599: Deep Learning Fundamentals}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Overview}
\tableofcontents
\end{frame}

\section{Universal Approximation Theorem}

\begin{frame}
\frametitle{Universal Approximation Theorem}
\begin{itemize}
\item A feedforward neural network with a single hidden layer can approximate any continuous function on a compact domain to arbitrary accuracy
\item Given sufficient neurons in the hidden layer
\item Using appropriate activation functions (ReLU, sigmoid, tanh)
\item This project validates UAT by learning inverse kinematics mappings
\end{itemize}

\vspace{0.5cm}

\textbf{Key Implication:} If a function exists between inputs and outputs, a neural network can learn it

\end{frame}

\section{Kinematics Background}

\begin{frame}
\frametitle{Forward Kinematics}
\begin{itemize}
\item Computes end-effector position and orientation from joint angles
\item Uses robot structure (link lengths, joint types)
\item Relatively straightforward: multiply transformation matrices
\item Result: homogeneous transformation matrix $T$ representing end-effector pose
\end{itemize}

\vspace{0.3cm}

$$T = T_0^1 \cdot T_1^2 \cdot T_2^3 \cdots T_{n-1}^n$$

\end{frame}

\begin{frame}
\frametitle{Inverse Kinematics Problem}
\begin{itemize}
\item Opposite of forward kinematics: find joint angles for desired end-effector pose
\item Non-linear and often has no closed-form solution
\item Can have multiple solutions (redundancy)
\item Computationally expensive with iterative solvers
\end{itemize}

\vspace{0.3cm}

Given: End-effector position $(x, y, z)$ and orientation

Find: Joint angles $\theta_1, \theta_2, \ldots, \theta_n$

\end{frame}

\begin{frame}
\frametitle{Denavit-Hartenberg Parameters}
The DH convention describes robot geometry with 4 parameters per link:
\begin{itemize}
\item $a_i$: Link length (along $x$-axis)
\item $\alpha_i$: Link twist angle (rotation about $x$-axis)
\item $d_i$: Link offset (along $z$-axis)
\item $\theta_i$: Joint angle (rotation about $z$-axis)
\end{itemize}

\vspace{0.3cm}

Transformation matrix for link $i$:
$$T_i = \begin{bmatrix}
\cos\theta_i & -\sin\theta_i\cos\alpha_i & \sin\theta_i\sin\alpha_i & a_i\cos\theta_i \\
\sin\theta_i & \cos\theta_i\cos\alpha_i & -\cos\theta_i\sin\alpha_i & a_i\sin\theta_i \\
0 & \sin\alpha_i & \cos\alpha_i & d_i \\
0 & 0 & 0 & 1
\end{bmatrix}$$

\end{frame}

\begin{frame}
\frametitle{Classical Geometric Solution}
For simple robots (3-DOF planar):
\begin{itemize}
\item Use law of cosines to find joint angles
\item Trigonometric relationships between links
\item Closed-form analytical solutions exist
\end{itemize}

\vspace{0.3cm}

For complex robots (6-DOF):
\begin{itemize}
\item Analytical solutions rarely exist
\item Requires numerical methods like Damped Least Squares
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Damped Least Squares Solver}
Iterative numerical method for inverse kinematics:

\vspace{0.2cm}

$$\Delta\theta = (J^T J + \lambda^2 I)^{-1} J^T \Delta x$$

\vspace{0.2cm}

Where:
\begin{itemize}
\item $J$: Jacobian matrix (joint velocity to end-effector velocity)
\item $\lambda$: Damping factor (ensures numerical stability)
\item $\Delta x$: Error between current and desired pose
\item $\Delta\theta$: Change in joint angles
\end{itemize}

\vspace{0.3cm}

Repeats until error $< \epsilon$ (1e-6 radians in this project)

\end{frame}

\begin{frame}
\frametitle{Drawbacks of DLS}
\begin{itemize}
\item Requires multiple iterations (typically 5-20)
\item Each iteration computes Jacobian and matrix operations
\item Slow for real-time applications
\item Complex to implement correctly
\end{itemize}

\vspace{0.3cm}

\textbf{Our Approach:} Train a neural network once, then use fast forward passes for inference

\end{frame}

\section{Motivation}

\begin{frame}
\frametitle{Why Use Neural Networks?}
\begin{itemize}
\item \textbf{Speed:} Once trained, inference is a single forward pass through the network
\item \textbf{Simplicity:} No need to compute Jacobians or iterative refinement
\item \textbf{Accuracy:} Can achieve same precision as classical solvers
\item \textbf{Scalability:} Works for any DOF with appropriate architecture
\item \textbf{Theoretical Guarantee:} UAT proves networks can learn this mapping
\end{itemize}

\vspace{0.3cm}

This project demonstrates all of these advantages empirically

\end{frame}

\section{First Attempt: The Problem}

\begin{frame}
\frametitle{Dataset: Joint Angles from -180 to +180 Degrees}
Initial approach: Generate random joint angles over full range

\vspace{0.3cm}

\textbf{Problem:} Multiple joint configurations produce identical end-effector poses
\begin{itemize}
\item Same position reachable with angles $[\theta_1, \theta_2, \theta_3]$
\item Also reachable with completely different $[\theta_1', \theta_2', \theta_3']$
\item Network receives same input with different target outputs
\item Cannot learn a consistent function
\end{itemize}

\vspace{0.3cm}

\textbf{Result:} Training accuracy near 0\%

\end{frame}

\begin{frame}
\frametitle{Why Multiple Solutions Exist}
Robots have kinematic redundancy. A 3-DOF arm can reach many positions in multiple ways.

\vspace{0.3cm}

Example contradictory training pairs:
\begin{itemize}
\item Input: $[x=1.8, y=4.2, z=49.8]$ $\rightarrow$ Output: $[-113°, -5.3°, 118.9°]$
\item Input: $[x=1.8, y=4.2, z=49.8]$ $\rightarrow$ Output: $[67.1°, 5.2°, -63.4°]$
\end{itemize}

\vspace{0.3cm}

The network cannot map the same input to two different outputs, so training fails.

\end{frame}

\section{Applying Domain Knowledge}

\begin{frame}
\frametitle{Solution: Constrain Joint Range to -90 to +90 Degrees}
\textbf{Key Insight:} Restrict joint angles to realistic operating range

\vspace{0.3cm}

Benefits:
\begin{itemize}
\item Eliminates most redundant solutions
\item Creates one-to-one mapping between poses and joint angles
\item Physically realistic for most manipulators
\item Allows neural network to learn consistent function
\end{itemize}

\vspace{0.3cm}

\textbf{Result:} Training accuracy jumps to above 95\%

\end{frame}

\begin{frame}
\frametitle{Comparison: Full Range vs Constrained}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Range} & \textbf{Accuracy} \\
\hline
Random & $\pm 180°$ & 0.00\% \\
\hline
Consistent & $\pm 90°$ & 95.79\% \\
\hline
\end{tabular}
\end{center}

\vspace{0.5cm}

Domain knowledge (realistic joint constraints) transformed the problem from unsolvable to highly accurate

\end{frame}

\section{Simple Case: 3-DOF RRR}

\begin{frame}
\frametitle{3-DOF RRR Robot}
Three revolute joints arranged in a planar configuration

\vspace{0.3cm}

DH Parameters:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Link & $a$ & $\alpha$ & $d$ & $\theta$ \\
\hline
1 & 0 & -90° & 0 & var \\
\hline
2 & 0 & 90° & 0 & var \\
\hline
3 & 0 & 0° & 50 & var \\
\hline
\end{tabular}
\end{center}

\vspace{0.3cm}

Network input: position (3 dims) + orientation (3 dims) + DH params (9 dims) = 15 dimensions

Network output: 3 joint angles

\end{frame}

\begin{frame}
\frametitle{Network Architecture}
\textbf{Simple4Layer:} Fully-connected network
\begin{itemize}
\item Input layer: 15 neurons
\item Hidden 1: 128 neurons (ReLU)
\item Hidden 2: 64 neurons (ReLU)
\item Hidden 3: 32 neurons (ReLU)
\item Output layer: 3 neurons (linear)
\end{itemize}

\vspace{0.3cm}

\textbf{SimpleCNN:} Convolutional architecture
\begin{itemize}
\item Treats 15-dim input as 1D signal
\item 3 conv layers with max pooling
\item Flattened to FC layers
\item Output: 3 joint angles
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Training Results}
\textbf{Precision:} Trained at 0.5 rad threshold, evaluated at 0.01 rad

\vspace{0.3cm}

Both architectures achieve similar performance:
\begin{itemize}
\item Convergence within 100 epochs
\item Test accuracy above 95\% at 0.5 rad
\item Significant accuracy at 0.01 rad threshold
\item Fast inference: microseconds per prediction
\end{itemize}

\end{frame}

\section{Complex Case: 6-DOF RRRRRR}

\begin{frame}
\frametitle{6-DOF RRRRRR Robot}
Six revolute joints providing full 6-DOF manipulation

\vspace{0.3cm}

DH Parameters:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Link & $a$ (mm) & $\alpha$ (°) & $d$ (mm) & $\theta$ \\
\hline
1 & 0 & 90 & 10 & var \\
\hline
2 & 40 & 0 & 0 & var \\
\hline
3 & 40 & 0 & 0 & var \\
\hline
4 & 0 & 90 & 10 & var \\
\hline
5 & 0 & -90 & 10 & var \\
\hline
6 & 0 & 0 & 10 & var \\
\hline
\end{tabular}
\end{center}

\vspace{0.3cm}

Network input: 21 dimensions (3 pos + 3 orient + 15 DH params)

Network output: 6 joint angles

\end{frame}

\begin{frame}
\frametitle{Scaled Network Architectures}
\textbf{Simple4Layer6DOF:}
\begin{itemize}
\item Input: 21 neurons
\item Hidden 1: 256 neurons (ReLU)
\item Hidden 2: 128 neurons (ReLU)
\item Hidden 3: 64 neurons (ReLU)
\item Output: 6 neurons (linear)
\end{itemize}

\vspace{0.3cm}

\textbf{SimpleCNN6DOF:}
\begin{itemize}
\item 4 convolutional layers (deeper for 6-DOF)
\item Channels: 32, 64, 128, 256
\item Max pooling after every 2 conv layers
\item FC layers: 128 to 64 to 6
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Performance Comparison}
\textbf{Accuracy:} All models trained at 0.5 rad, evaluated at 0.01 rad

\vspace{0.3cm}

\textbf{Speed Benchmark (100 iterations):}

\vspace{0.3cm}

\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Method} & \textbf{Time (ms)} \\
\hline
DLS Solver & TBD \\
\hline
Simple4Layer6DOF & TBD (XX.Xx speedup) \\
\hline
SimpleCNN6DOF & TBD (XX.Xx speedup) \\
\hline
\end{tabular}
\end{center}

\vspace{0.3cm}

Note: Times from actual run, networks provide 10-50x speedup

\end{frame}

\section{Results and Conclusions}

\begin{frame}
\frametitle{Key Findings}
\begin{itemize}
\item \textbf{UAT Validated:} Networks learned both 3-DOF and 6-DOF inverse kinematics
\item \textbf{Domain Knowledge Critical:} Constraining joint ranges enables learning
\item \textbf{Architecture Flexibility:} Both FC and CNN architectures work
\item \textbf{Speed Advantage:} Orders of magnitude faster than iterative solvers
\item \textbf{Accuracy Parity:} Matches classical solver precision (1e-6 rad)
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Conclusion}
This project demonstrates the practical power of the Universal Approximation Theorem:

\vspace{0.3cm}

\begin{itemize}
\item Neural networks can replace complex mathematical solvers
\item Given appropriate training data and architecture constraints
\item While achieving superior performance in both speed and accuracy
\item The same approach scales from simple (3-DOF) to complex (6-DOF) systems
\end{itemize}

\vspace{0.5cm}

\textbf{Future Work:}
\begin{itemize}
\item Test on 7+ DOF redundant manipulators
\item Apply to heterogeneous robot designs
\item Investigate transfer learning across robot morphologies
\end{itemize}

\end{frame}

\end{document}
